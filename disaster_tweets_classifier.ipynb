{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c6ff82-5cba-43ec-8f4a-3ed77459fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import trax.fastmath.numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import optimizers\n",
    "from trax.supervised import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b984ecf-74d4-42df-994f-85fc58aa10e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80ec378a-e2f1-444e-bfd2-b519cec20e87",
   "metadata": {},
   "source": [
    "## Loading tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9e9d5a-97d4-4d3d-a262-f0bbd9620e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PCT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e08e71-2878-47c3-b7b3-44ff071ef391",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_tweets = pd.read_csv('train.csv')\n",
    "all_test_tweets = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f459df4-c39c-4256-b882-6a2b4ef68c91",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0247cf8-cac1-449a-a9a4-b8124fc6d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet, remove_stopwords=False, stem=False):\n",
    "    # Remove hyper-links\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Remove stock market tickers\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # Remove old style tweet text RT\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    if remove_stopwords\n",
    "    \n",
    "    \n",
    "    # Tokenize tweet\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    return [word for word in tokenizer.tokenize(tweet) if word not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac17ea14-2ec8-4f20-8dc1-b3d97c5668e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_tweets['clean'] = all_train_tweets.text.map(process_tweet, args(True,))\n",
    "all_test_tweets['clean'] = all_test_tweets.text.map(process_tweet, args(True,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c6101e-845e-43ca-8c71-0b710041f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary\n",
    "vocab = {\n",
    "    '__PAD__': 0,\n",
    "    '__<e>__': 1,\n",
    "    '__UNK__': 2,\n",
    "}\n",
    "for tweet in all_train_tweets.clean.to_list():\n",
    "    for word in tweet:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "# Tweet to tensor\n",
    "def tweet_to_tensor(tweet, vocab):\n",
    "    return [vocab.get(token, vocab['__UNK__']) for token in tweet]\n",
    "\n",
    "all_train_tweets['clean'] = all_train_tweets.clean.apply(tweet_to_tensor, args=(vocab,))\n",
    "all_test_tweets['clean'] = all_test_tweets.clean.apply(tweet_to_tensor, args=(vocab,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bfbdc4e-d292-4569-9fdd-30dbcead0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation split\n",
    "all_pos_train = all_train_tweets.loc[all_train_tweets.target == 1]\n",
    "all_neg_train = all_train_tweets.loc[all_train_tweets.target == 0]\n",
    "\n",
    "pos_cut_idx = int(all_pos_train.shape[0] * (1 - VAL_PCT))\n",
    "pos_val = all_pos_train.iloc[pos_cut_idx:]\n",
    "pos_train = all_pos_train.iloc[:pos_cut_idx]\n",
    "\n",
    "neg_cut_idx = int(all_neg_train.shape[0] * (1 - VAL_PCT))\n",
    "neg_val = all_neg_train.iloc[neg_cut_idx:]\n",
    "neg_train = all_neg_train.iloc[:neg_cut_idx] \n",
    "\n",
    "all_train = pd.concat([pos_train, neg_train])\n",
    "all_val = pd.concat([pos_val, neg_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c3ca1d-5020-4679-a33a-842bbbfe7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_pos, data_neg, batch_size, loop=False):\n",
    "    len_pos = len(data_pos)\n",
    "    len_neg = len(data_neg)\n",
    "    \n",
    "    pos_idx_lines =  list(range(len_pos))\n",
    "    neg_idx_lines = list(range(len_neg))\n",
    "    \n",
    "    pos_idx = 0\n",
    "    neg_idx = 0\n",
    "    \n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    random.shuffle(pos_idx_lines)\n",
    "    random.shuffle(neg_idx_lines)\n",
    "    \n",
    "    stop = False\n",
    "    \n",
    "    while not stop:\n",
    "        batch = []\n",
    "        targets = []\n",
    "        max_len = 0\n",
    "        for i in range(n_to_take):\n",
    "            if pos_idx >= len_pos or neg_idx >= len_neg:\n",
    "                if not loop:\n",
    "                    stop = True\n",
    "                    break\n",
    "                if pos_idx >= len_pos:\n",
    "                    pos_idx = 0\n",
    "                    random.shuffle(pos_idx_lines)\n",
    "                if neg_idx >= len_neg:\n",
    "                    neg_idx = 0\n",
    "                    random.shuffle(neg_idx_lines)\n",
    "                    \n",
    "            pos_elem = data_pos[pos_idx]\n",
    "            batch.append(pos_elem)\n",
    "            targets.append(1)\n",
    "            if len(pos_elem) > max_len:\n",
    "                max_len = len(pos_elem)\n",
    "\n",
    "            neg_elem = data_neg[neg_idx]\n",
    "            batch.append(neg_elem)\n",
    "            targets.append(0)\n",
    "            if len(neg_elem) > max_len:\n",
    "                max_len = len(neg_elem)\n",
    "\n",
    "            pos_idx += 1\n",
    "            neg_idx += 1\n",
    "                \n",
    "        if stop:\n",
    "            break\n",
    "            \n",
    "        pos_idx += n_to_take\n",
    "        neg_idx += n_to_take\n",
    "        \n",
    "        # padding\n",
    "        for elem in batch:\n",
    "            elem += [0] * (max_len - len(elem))\n",
    "            \n",
    "        example_weights = np.array([1] * (n_to_take * 2))\n",
    "            \n",
    "        yield np.array(batch), np.array(targets), example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5d729e-e7e4-41be-b2af-d66c80d8d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(batch_size, train_pos, train_neg):\n",
    "    return data_generator(\n",
    "        data_pos=train_pos,\n",
    "        data_neg=train_neg,\n",
    "        batch_size=batch_size,\n",
    "        loop=True\n",
    "    )\n",
    "def val_generator(batch_size, val_pos, val_neg):\n",
    "    return data_generator(\n",
    "        data_pos=val_pos,\n",
    "        data_neg=val_neg,\n",
    "        batch_size=batch_size,\n",
    "        loop=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5aa0f-b626-4b6d-866f-030888d51b45",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01397f-c597-45cf-bdec-5af624faea60",
   "metadata": {},
   "source": [
    "#### First trail, word embeddings with a single hidden layer. Should suffice for sentiment analysis tasks like this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd41d883-8570-43ff-a08e-0b644aad32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension. Something to experiemnt with.\n",
    "EMBED_DIM = 256\n",
    "BATCH_SIZE = 32\n",
    "OUTPUT_DIR = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1f05a59-4b1a-4b23-b8b9-18ca91eba596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(vocab_size, embedding_dim):\n",
    "    return tl.Serial(\n",
    "        tl.Embedding(\n",
    "            vocab_size=vocab_size,\n",
    "            d_feature=embedding_dim,\n",
    "        ),\n",
    "        tl.Mean(axis=1),\n",
    "        tl.Dense(n_units=2),\n",
    "        tl.LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4058fd9-8f80-4a55-9e4a-08502ecb63ce",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e551fe-333c-4506-906a-34f4a876bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_eval_tasks(train_pos, train_neg, val_pos, val_neg, batch_size):\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator(\n",
    "            batch_size=batch_size,\n",
    "            train_pos=train_pos,\n",
    "            train_neg=train_neg\n",
    "        ),\n",
    "        loss_layer=tl.CrossEntropyLoss(),\n",
    "        optimizer=optimizers.Adam(0.01),\n",
    "        n_steps_per_checkpoint=10,\n",
    "    )\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=val_generator(\n",
    "            batch_size=batch_size,\n",
    "            val_pos=val_pos,\n",
    "            val_neg=val_neg\n",
    "        ),\n",
    "        metrics=[\n",
    "            tl.CrossEntropyLoss(),\n",
    "            tl.Accuracy(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return train_task, eval_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25593b9b-778c-4d11-89cd-bbd5b103f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier(len(vocab), EMBED_DIM)\n",
    "train_task, eval_task = get_train_eval_tasks(\n",
    "    train_pos=pos_train.clean.to_list(),\n",
    "    train_neg=neg_train.clean.to_list(),\n",
    "    val_pos=pos_val.clean.to_list(),\n",
    "    val_neg=neg_val.clean.to_list(),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b46a90c6-8598-45d4-8477-bc0d703d0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_task, eval_task, n_steps, output_dir):\n",
    "    training_loop = training.Loop(\n",
    "        model=model,\n",
    "        tasks=train_task,\n",
    "        eval_tasks=eval_task,\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "    \n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ca4d017-41a2-411f-b2af-d422807aa2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atom/work/nlp/nlpenv/lib/python3.8/site-packages/jax/_src/lib/xla_bridge.py:514: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    210: Ran 10 train steps in 4.96 secs\n",
      "Step    210: train CrossEntropyLoss |  0.28367159\n",
      "Step    210: eval  CrossEntropyLoss |  0.69616920\n",
      "Step    210: eval          Accuracy |  0.62500000\n",
      "\n",
      "Step    220: Ran 10 train steps in 2.37 secs\n",
      "Step    220: train CrossEntropyLoss |  0.19699201\n",
      "Step    220: eval  CrossEntropyLoss |  0.54470193\n",
      "Step    220: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step    230: Ran 10 train steps in 2.05 secs\n",
      "Step    230: train CrossEntropyLoss |  0.27555084\n",
      "Step    230: eval  CrossEntropyLoss |  0.14237022\n",
      "Step    230: eval          Accuracy |  0.96875000\n",
      "\n",
      "Step    240: Ran 10 train steps in 2.32 secs\n",
      "Step    240: train CrossEntropyLoss |  0.16946909\n",
      "Step    240: eval  CrossEntropyLoss |  0.45608792\n",
      "Step    240: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step    250: Ran 10 train steps in 1.50 secs\n",
      "Step    250: train CrossEntropyLoss |  0.11694790\n",
      "Step    250: eval  CrossEntropyLoss |  0.49138933\n",
      "Step    250: eval          Accuracy |  0.78125000\n",
      "\n",
      "Step    260: Ran 10 train steps in 1.90 secs\n",
      "Step    260: train CrossEntropyLoss |  0.29203230\n",
      "Step    260: eval  CrossEntropyLoss |  0.35611185\n",
      "Step    260: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step    270: Ran 10 train steps in 1.59 secs\n",
      "Step    270: train CrossEntropyLoss |  0.33030081\n",
      "Step    270: eval  CrossEntropyLoss |  0.42612016\n",
      "Step    270: eval          Accuracy |  0.87500000\n",
      "\n",
      "Step    280: Ran 10 train steps in 2.00 secs\n",
      "Step    280: train CrossEntropyLoss |  0.53530467\n",
      "Step    280: eval  CrossEntropyLoss |  0.35100174\n",
      "Step    280: eval          Accuracy |  0.81250000\n",
      "\n",
      "Step    290: Ran 10 train steps in 4.17 secs\n",
      "Step    290: train CrossEntropyLoss |  0.32750878\n",
      "Step    290: eval  CrossEntropyLoss |  0.72660863\n",
      "Step    290: eval          Accuracy |  0.65625000\n",
      "\n",
      "Step    300: Ran 10 train steps in 1.53 secs\n",
      "Step    300: train CrossEntropyLoss |  0.23715687\n",
      "Step    300: eval  CrossEntropyLoss |  0.70789540\n",
      "Step    300: eval          Accuracy |  0.65625000\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(\n",
    "    model=model,\n",
    "    train_task=train_task,\n",
    "    eval_task=[eval_task],\n",
    "    n_steps=100,\n",
    "    output_dir=OUTPUT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b9512-af8f-4d81-b222-e9e7d7a33d19",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9457493-9b27-4bf2-8905-68904c4bbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "max_len = all_test_tweets.clean.map(len).max().item()\n",
    "\n",
    "def pad(tweet, max_len):\n",
    "    return tweet + ([0] * (max_len - len(tweet)))\n",
    "\n",
    "all_test_tweets['clean'] = all_test_tweets.clean.apply(pad, args=(max_len,))\n",
    "\n",
    "preds = training_loop.eval_model(np.array(all_test_tweets.clean.to_list()))\n",
    "target = np.array([pred[1] > pred[0] for pred in preds]).astype(np.float32)\n",
    "all_test_tweets['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d161d715-d548-4d53-8ea6-744766986621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "all_test_tweets[['id', 'target']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ce7da-e31d-4e9d-ae8f-bfa321653078",
   "metadata": {},
   "source": [
    "## Experiment log:\n",
    "- First thing, we tried a simple model with an embedding layer, followed by a mean layer, then the output layers. Stopwords were not removed and no stemming was performed. Accuracy was varying between runs, ranging between 0.62500000 0.96875000. Something is definitely wrong.\n",
    "- Next thing to try, is word removal alone, then stemming alone, then both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee66474-fe13-4ef7-b798-eb28d320ae69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
