{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcfe116f-8b10-4002-88da-bc4cc247b550",
   "metadata": {},
   "source": [
    "## Experiment log:\n",
    "- First, we took only the tweet text, without keywords or locations, and tried a simple model with an embedding layer, followed by a mean layer, then the output layers. Stopwords were not removed and no stemming was performed. Accuracy was varying between runs, ranging between 0.625 0.96875. We can do better.\n",
    "- Next,we tried removing stopwords. Accuracy did not peak as high as it was before, but the variance was less. Acuracy ranged between 0.46875 and 0.8125.\n",
    "- We tried stemming alone. Same as removing stopwords. Acuracy ranged between 0.5 and 0.8125.\n",
    "- Both stemming and removing stopwords made the training smoother, with the accuracy less fluctuating and almost steadily increasing. Range was between 0.46875 and 0.75.\n",
    "- After looking at kewyords, they seem helpful. They are the one or few words that are the main focus of the tweet. We could use that.\n",
    "- We tried just appending the keywords at the end of the original tweets, althought they are there anyway. We thought that would help the model pay more attention to the keyword and that would help capturing the sentiment of the tweet. It could have done that, but we did not see any significant increase in accuracy, it was between 0.5 and 0.8125. We need to think of another way to include them.\n",
    "-  Next, we tried an Embedding, LSTM, Fn (select_last), Dense (output), LogSoftmax/Sigmoid/Relu architecture. It was terrible! The accuracy was 0.5 most of the time, and even dropped to 0.40625 briefly. \n",
    "- We tried then to be creative :D we created two branches of Embedding, Mean, Dense, and LogSoftmax, one to process the tweet text and the other to process the keyword alone, then we averaged the log-softmax scores. The results were much better! The accuracy peaked at 1.0 briefly. We then did some tweaking of the learning rate and number of iterations not just to get higher accuracy but also to make it more steady and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c6ff82-5cba-43ec-8f4a-3ed77459fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import trax.fastmath.numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import optimizers\n",
    "from trax.supervised import training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec378a-e2f1-444e-bfd2-b519cec20e87",
   "metadata": {},
   "source": [
    "## Loading tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9e9d5a-97d4-4d3d-a262-f0bbd9620e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PCT = 0.2\n",
    "MODEL_DIR = './model'\n",
    "OUTPUT_DIR = './output'\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e08e71-2878-47c3-b7b3-44ff071ef391",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_tweets = pd.read_csv('data/train.csv')\n",
    "all_test_tweets = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb931b3-10c0-4f66-b7da-e7466dcbdd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_tweets.loc[all_train_tweets.target == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e74d9944-dabc-4d9b-9ae6-352f3d8decd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                          text  target\n",
       "15  23     NaN      NaN                What's up man?       0\n",
       "16  24     NaN      NaN                 I love fruits       0\n",
       "17  25     NaN      NaN              Summer is lovely       0\n",
       "18  26     NaN      NaN             My car is so fast       0\n",
       "19  28     NaN      NaN  What a goooooooaaaaaal!!!!!!       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_tweets.loc[all_train_tweets.target == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e796db1-9ce7-4d88-b9f1-0624fa3f09dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31                       Birmingham\n",
       "32    Est. September 2012 - Bristol\n",
       "33                           AFRICA\n",
       "34                 Philadelphia, PA\n",
       "35                       London, UK\n",
       "Name: location, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_tweets.location.loc[~all_train_tweets.location.isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5bf63e-6403-439a-b89d-6160a92dfaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "sinking                  41\n",
       "damage                   41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_tweets.keyword.loc[~all_train_tweets.keyword.isna()].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa7e08-4482-4775-af3c-b08013a162bc",
   "metadata": {},
   "source": [
    "Keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dfebd12-c55f-44b2-8163-24db2467acca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C ablaze',\n",
       " 'We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw ablaze',\n",
       " '#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi ablaze',\n",
       " 'Crying out for more! Set me ablaze ablaze',\n",
       " 'On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N ablaze']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That is how the tweets looked like when we tried appending the keywords\n",
    "(all_train_tweets.text + ' ' + all_train_tweets.keyword.fillna('')).loc[~all_train_tweets.keyword.isna()].to_list()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f459df4-c39c-4256-b882-6a2b4ef68c91",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0247cf8-cac1-449a-a9a4-b8124fc6d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet, remove_stopwords=False, stem=False):\n",
    "    # Remove hyper-links\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Remove stock market tickers\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # Remove old style tweet text RT\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # Tokenize tweet\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet = [word for word in tokenizer.tokenize(tweet) if word not in string.punctuation]\n",
    "    if remove_stopwords:\n",
    "        tweet = [word for word in tweet if word not in stopwords_english]\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tweet = [stemmer.stem(word) for word in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac17ea14-2ec8-4f20-8dc1-b3d97c5668e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_tweets['text_clean'] = all_train_tweets.text.apply(process_tweet, args=(True, True))\n",
    "all_train_tweets['keyword_clean'] = all_train_tweets.keyword.fillna('__UNK__').apply(process_tweet, args=(True, True))\n",
    "\n",
    "all_test_tweets['text_clean'] = all_test_tweets.text.apply(process_tweet, args=(True, True))\n",
    "all_test_tweets['keyword_clean'] = all_test_tweets.keyword.fillna('__UNK__').apply(process_tweet, args=(True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f95b006-b208-407c-92f4-82ab7fdb5549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00     0.0\n",
       "0.25     6.0\n",
       "0.50     9.0\n",
       "0.75    12.0\n",
       "1.00    26.0\n",
       "Name: text_clean, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The purpose of this is to show how long the sequences we are dealing with.\n",
    "# The longer the sequence, the tricker it is to capture the whole meaning of the tweet.\n",
    "all_train_tweets.text_clean.map(lambda t: len(t)).quantile([0.0, 0.25, .50, 0.75, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c6101e-845e-43ca-8c71-0b710041f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary\n",
    "vocab = {\n",
    "    '__PAD__': 0,\n",
    "    '__</e>__': 1,\n",
    "    '__UNK__': 2,\n",
    "}\n",
    "for tweet in all_train_tweets.text_clean.to_list():\n",
    "    for word in tweet:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "# Tweet to tensor\n",
    "def tweet_to_tensor(tweet, vocab):\n",
    "    return [vocab.get(token, vocab['__UNK__']) for token in tweet] + [vocab['__</e>__']]\n",
    "\n",
    "all_train_tweets['text_clean'] = all_train_tweets.text_clean.apply(tweet_to_tensor, args=(vocab,))\n",
    "all_train_tweets['keyword_clean'] = all_train_tweets.keyword_clean.apply(tweet_to_tensor, args=(vocab,))\n",
    "\n",
    "all_test_tweets['text_clean'] = all_test_tweets.text_clean.apply(tweet_to_tensor, args=(vocab,))\n",
    "all_test_tweets['keyword_clean'] = all_test_tweets.keyword_clean.apply(tweet_to_tensor, args=(vocab,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bfbdc4e-d292-4569-9fdd-30dbcead0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation split\n",
    "all_pos_train = all_train_tweets.loc[all_train_tweets.target == 1]\n",
    "all_neg_train = all_train_tweets.loc[all_train_tweets.target == 0]\n",
    "\n",
    "pos_cut_idx = int(all_pos_train.shape[0] * (1 - VAL_PCT))\n",
    "pos_val = all_pos_train.iloc[pos_cut_idx:]\n",
    "pos_train = all_pos_train.iloc[:pos_cut_idx]\n",
    "\n",
    "neg_cut_idx = int(all_neg_train.shape[0] * (1 - VAL_PCT))\n",
    "neg_val = all_neg_train.iloc[neg_cut_idx:]\n",
    "neg_train = all_neg_train.iloc[:neg_cut_idx] \n",
    "\n",
    "all_train = pd.concat([pos_train, neg_train])\n",
    "all_val = pd.concat([pos_val, neg_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3c3ca1d-5020-4679-a33a-842bbbfe7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(text_pos, text_neg, keyword_pos, keyword_neg, batch_size, vocab, loop=False):\n",
    "    len_pos = len(text_pos)\n",
    "    len_neg = len(text_neg)\n",
    "    \n",
    "    pos_idx_lines =  list(range(len_pos))\n",
    "    neg_idx_lines = list(range(len_neg))\n",
    "    \n",
    "    pos_idx = 0\n",
    "    neg_idx = 0\n",
    "    \n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    random.shuffle(pos_idx_lines)\n",
    "    random.shuffle(neg_idx_lines)\n",
    "    \n",
    "    stop = False\n",
    "    \n",
    "    while not stop:\n",
    "        batch_text = []\n",
    "        batch_keyword = []\n",
    "        targets = []\n",
    "        max_len_text = 0\n",
    "        max_len_keyword = 0\n",
    "        for i in range(n_to_take):\n",
    "            if pos_idx >= len_pos or neg_idx >= len_neg:\n",
    "                if not loop:\n",
    "                    stop = True\n",
    "                    break\n",
    "                if pos_idx >= len_pos:\n",
    "                    pos_idx = 0\n",
    "                    random.shuffle(pos_idx_lines)\n",
    "                if neg_idx >= len_neg:\n",
    "                    neg_idx = 0\n",
    "                    random.shuffle(neg_idx_lines)\n",
    "                    \n",
    "            pos_text = text_pos[pos_idx]\n",
    "            pos_keyword = keyword_pos[pos_idx]\n",
    "            batch_text.append(pos_text)\n",
    "            batch_keyword.append(pos_keyword)\n",
    "            targets.append(1)\n",
    "            if len(pos_text) > max_len_text:\n",
    "                max_len_text = len(pos_text)\n",
    "            if len(pos_keyword) > max_len_keyword:\n",
    "                max_len_keyword = len(pos_keyword)\n",
    "\n",
    "            neg_text = text_neg[neg_idx]\n",
    "            neg_keyword = keyword_neg[neg_idx]\n",
    "            batch_text.append(neg_text)\n",
    "            batch_keyword.append(neg_keyword)\n",
    "            targets.append(0)\n",
    "            if len(neg_text) > max_len_text:\n",
    "                max_len_text = len(neg_text)\n",
    "            if len(neg_keyword) > max_len_keyword:\n",
    "                max_len_keyword = len(neg_keyword)\n",
    "\n",
    "            pos_idx += 1\n",
    "            neg_idx += 1\n",
    "                \n",
    "        if stop:\n",
    "            break\n",
    "            \n",
    "        pos_idx += n_to_take\n",
    "        neg_idx += n_to_take\n",
    "        \n",
    "        # padding\n",
    "        for elem in batch_text:\n",
    "            elem += [vocab['__PAD__']] * (max_len_text - len(elem))\n",
    "        for elem in batch_keyword:\n",
    "            elem += [vocab['__PAD__']] * (max_len_keyword - len(elem))\n",
    "            \n",
    "        example_weights = np.array([1] * (n_to_take * 2))\n",
    "            \n",
    "        yield np.array(batch_text), np.array(batch_keyword), np.array(targets), example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef5d729e-e7e4-41be-b2af-d66c80d8d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(batch_size, train_text_pos, train_text_neg, train_keyword_pos, train_keyword_neg, vocab):\n",
    "    return data_generator(\n",
    "        text_pos=train_text_pos,\n",
    "        text_neg=train_text_neg,\n",
    "        keyword_pos=train_keyword_pos,\n",
    "        keyword_neg=train_keyword_neg,\n",
    "        batch_size=batch_size,\n",
    "        vocab=vocab,\n",
    "        loop=True\n",
    "    )\n",
    "def val_generator(batch_size, val_text_pos, val_text_neg, val_keyword_pos, val_keyword_neg, vocab):\n",
    "    return data_generator(\n",
    "        text_pos=val_text_pos,\n",
    "        text_neg=val_text_neg,\n",
    "        keyword_pos=val_keyword_pos,\n",
    "        keyword_neg=val_keyword_neg,\n",
    "        batch_size=batch_size,\n",
    "        vocab=vocab,\n",
    "        loop=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5aa0f-b626-4b6d-866f-030888d51b45",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01397f-c597-45cf-bdec-5af624faea60",
   "metadata": {},
   "source": [
    "#### First trail, word embeddings with a single hidden layer. Should suffice for sentiment analysis tasks like this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd41d883-8570-43ff-a08e-0b644aad32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension. Something to experiemnt with.\n",
    "EMBED_DIM = 256\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a1f05a59-4b1a-4b23-b8b9-18ca91eba596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_last(seq):\n",
    "    return seq[:,-1,:]\n",
    "    \n",
    "def calc_avg(text_score, keyword_score):\n",
    "    return (text_score + keyword_score) / 2\n",
    "\n",
    "\n",
    "def classifier(vocab_size, embedding_dim):\n",
    "    embedding =  tl.Embedding(\n",
    "        vocab_size=vocab_size,\n",
    "        d_feature=embedding_dim,\n",
    "    )\n",
    "    l_mean = tl.Mean(axis=1)\n",
    "    lstm = tl.LSTM(embedding_dim)\n",
    "    l_fn = tl.Fn('select_last', select_last)\n",
    "    dense1 = tl.Dense(n_units=embedding_dim)\n",
    "    dense2 = tl.Dense(n_units=2)\n",
    "    logsoftmax = tl.LogSoftmax()\n",
    "    relu = tl.Relu()\n",
    "    sigmoid = tl.Sigmoid()\n",
    "    \n",
    "    return tl.Serial(\n",
    "        tl.Parallel(\n",
    "            tl.Serial(\n",
    "                tl.Embedding(\n",
    "                    vocab_size=vocab_size,\n",
    "                    d_feature=embedding_dim,\n",
    "                ),\n",
    "                tl.Mean(axis=1),\n",
    "                tl.Dense(n_units=2),\n",
    "                tl.LogSoftmax(),\n",
    "            ),\n",
    "            tl.Serial(\n",
    "                tl.Embedding(\n",
    "                    vocab_size=vocab_size,\n",
    "                    d_feature=embedding_dim,\n",
    "                ),\n",
    "                tl.Mean(axis=1),\n",
    "                tl.Dense(n_units=2),\n",
    "                tl.LogSoftmax(),\n",
    "            ),\n",
    "        ),\n",
    "        tl.Fn('avg', calc_avg)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4058fd9-8f80-4a55-9e4a-08502ecb63ce",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "90e551fe-333c-4506-906a-34f4a876bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_eval_tasks(\n",
    "    train_text_pos, train_text_neg, train_keyword_pos, train_keyword_neg,\n",
    "    val_text_pos, val_text_neg, val_keyword_pos, val_keyword_neg,\n",
    "    batch_size, vocab\n",
    "):\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator(\n",
    "            batch_size=batch_size,\n",
    "            train_text_pos=train_text_pos,\n",
    "            train_text_neg=train_text_neg,\n",
    "            train_keyword_pos=train_keyword_pos,\n",
    "            train_keyword_neg=train_keyword_neg,\n",
    "            vocab=vocab,\n",
    "        ),\n",
    "        loss_layer=tl.CrossEntropyLoss(),\n",
    "        optimizer=optimizers.Adam(0.001),\n",
    "        n_steps_per_checkpoint=10,\n",
    "    )\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=val_generator(\n",
    "            batch_size=batch_size,\n",
    "            val_text_pos=val_text_pos,\n",
    "            val_text_neg=val_text_neg,\n",
    "            val_keyword_pos=val_keyword_pos,\n",
    "            val_keyword_neg=val_keyword_neg,\n",
    "            vocab=vocab,\n",
    "        ),\n",
    "        metrics=[\n",
    "            tl.CrossEntropyLoss(),\n",
    "            tl.Accuracy(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return train_task, eval_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25593b9b-778c-4d11-89cd-bbd5b103f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier(len(vocab), EMBED_DIM)\n",
    "train_task, eval_task = get_train_eval_tasks(\n",
    "    train_text_pos=pos_train.text_clean.to_list(),\n",
    "    train_text_neg=neg_train.text_clean.to_list(),\n",
    "    train_keyword_pos=pos_train.keyword_clean.to_list(),\n",
    "    train_keyword_neg=neg_train.keyword_clean.to_list(),\n",
    "    val_text_pos=pos_val.text_clean.to_list(),\n",
    "    val_text_neg=neg_val.text_clean.to_list(),\n",
    "    val_keyword_pos=pos_val.keyword_clean.to_list(),\n",
    "    val_keyword_neg=neg_val.keyword_clean.to_list(),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    vocab=vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9045393-e4c9-4297-b5bf-31a0ea352427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial_in2[\n",
       "  Parallel_in2_out2[\n",
       "    Serial[\n",
       "      Embedding_11964_256\n",
       "      Mean\n",
       "      Dense_2\n",
       "      LogSoftmax\n",
       "    ]\n",
       "    Serial[\n",
       "      Embedding_11964_256\n",
       "      Mean\n",
       "      Dense_2\n",
       "      LogSoftmax\n",
       "    ]\n",
       "  ]\n",
       "  avg_in2\n",
       "]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b46a90c6-8598-45d4-8477-bc0d703d0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_task, eval_task, n_steps, output_dir):\n",
    "    training_loop = training.Loop(\n",
    "        model=model,\n",
    "        tasks=train_task,\n",
    "        eval_tasks=eval_task,\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "    \n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cc14300-ae1b-499e-935d-6d987671c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the model directory, otherwise old checkpoints might fail to load and training would fail\n",
    "for filename in os.listdir(MODEL_DIR):\n",
    "    file_path = os.path.join(MODEL_DIR, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9ca4d017-41a2-411f-b2af-d422807aa2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 6126596\n",
      "Step      1: Ran 1 train steps in 2.25 secs\n",
      "Step      1: train CrossEntropyLoss |  0.68846506\n",
      "Step      1: eval  CrossEntropyLoss |  0.68605191\n",
      "Step      1: eval          Accuracy |  0.78125000\n",
      "\n",
      "Step     10: Ran 9 train steps in 9.59 secs\n",
      "Step     10: train CrossEntropyLoss |  0.69075823\n",
      "Step     10: eval  CrossEntropyLoss |  0.68003577\n",
      "Step     10: eval          Accuracy |  0.84375000\n",
      "\n",
      "Step     20: Ran 10 train steps in 4.49 secs\n",
      "Step     20: train CrossEntropyLoss |  0.68754375\n",
      "Step     20: eval  CrossEntropyLoss |  0.65674168\n",
      "Step     20: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step     30: Ran 10 train steps in 6.76 secs\n",
      "Step     30: train CrossEntropyLoss |  0.71607256\n",
      "Step     30: eval  CrossEntropyLoss |  0.68104517\n",
      "Step     30: eval          Accuracy |  0.59375000\n",
      "\n",
      "Step     40: Ran 10 train steps in 2.88 secs\n",
      "Step     40: train CrossEntropyLoss |  0.68957978\n",
      "Step     40: eval  CrossEntropyLoss |  0.68974495\n",
      "Step     40: eval          Accuracy |  0.50000000\n",
      "\n",
      "Step     50: Ran 10 train steps in 4.57 secs\n",
      "Step     50: train CrossEntropyLoss |  0.69915557\n",
      "Step     50: eval  CrossEntropyLoss |  0.67653996\n",
      "Step     50: eval          Accuracy |  0.65625000\n",
      "\n",
      "Step     60: Ran 10 train steps in 2.86 secs\n",
      "Step     60: train CrossEntropyLoss |  0.69850022\n",
      "Step     60: eval  CrossEntropyLoss |  0.66133583\n",
      "Step     60: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step     70: Ran 10 train steps in 2.24 secs\n",
      "Step     70: train CrossEntropyLoss |  0.68855274\n",
      "Step     70: eval  CrossEntropyLoss |  0.68052286\n",
      "Step     70: eval          Accuracy |  0.59375000\n",
      "\n",
      "Step     80: Ran 10 train steps in 2.98 secs\n",
      "Step     80: train CrossEntropyLoss |  0.70177996\n",
      "Step     80: eval  CrossEntropyLoss |  0.67197412\n",
      "Step     80: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step     90: Ran 10 train steps in 2.11 secs\n",
      "Step     90: train CrossEntropyLoss |  0.68970394\n",
      "Step     90: eval  CrossEntropyLoss |  0.67952710\n",
      "Step     90: eval          Accuracy |  0.68750000\n",
      "\n",
      "Step    100: Ran 10 train steps in 2.63 secs\n",
      "Step    100: train CrossEntropyLoss |  0.67834210\n",
      "Step    100: eval  CrossEntropyLoss |  0.67457807\n",
      "Step    100: eval          Accuracy |  0.53125000\n",
      "\n",
      "Step    110: Ran 10 train steps in 2.24 secs\n",
      "Step    110: train CrossEntropyLoss |  0.67547572\n",
      "Step    110: eval  CrossEntropyLoss |  0.64590621\n",
      "Step    110: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step    120: Ran 10 train steps in 2.20 secs\n",
      "Step    120: train CrossEntropyLoss |  0.66536635\n",
      "Step    120: eval  CrossEntropyLoss |  0.66807419\n",
      "Step    120: eval          Accuracy |  0.65625000\n",
      "\n",
      "Step    130: Ran 10 train steps in 2.24 secs\n",
      "Step    130: train CrossEntropyLoss |  0.66191626\n",
      "Step    130: eval  CrossEntropyLoss |  0.69142538\n",
      "Step    130: eval          Accuracy |  0.59375000\n",
      "\n",
      "Step    140: Ran 10 train steps in 3.27 secs\n",
      "Step    140: train CrossEntropyLoss |  0.68490231\n",
      "Step    140: eval  CrossEntropyLoss |  0.63612610\n",
      "Step    140: eval          Accuracy |  0.78125000\n",
      "\n",
      "Step    150: Ran 10 train steps in 2.12 secs\n",
      "Step    150: train CrossEntropyLoss |  0.66314071\n",
      "Step    150: eval  CrossEntropyLoss |  0.66885388\n",
      "Step    150: eval          Accuracy |  0.71875000\n",
      "\n",
      "Step    160: Ran 10 train steps in 3.12 secs\n",
      "Step    160: train CrossEntropyLoss |  0.64997548\n",
      "Step    160: eval  CrossEntropyLoss |  0.64405513\n",
      "Step    160: eval          Accuracy |  0.68750000\n",
      "\n",
      "Step    170: Ran 10 train steps in 4.40 secs\n",
      "Step    170: train CrossEntropyLoss |  0.66782087\n",
      "Step    170: eval  CrossEntropyLoss |  0.66091514\n",
      "Step    170: eval          Accuracy |  0.59375000\n",
      "\n",
      "Step    180: Ran 10 train steps in 2.33 secs\n",
      "Step    180: train CrossEntropyLoss |  0.65570676\n",
      "Step    180: eval  CrossEntropyLoss |  0.64650726\n",
      "Step    180: eval          Accuracy |  0.68750000\n",
      "\n",
      "Step    190: Ran 10 train steps in 2.97 secs\n",
      "Step    190: train CrossEntropyLoss |  0.66449285\n",
      "Step    190: eval  CrossEntropyLoss |  0.65204459\n",
      "Step    190: eval          Accuracy |  0.65625000\n",
      "\n",
      "Step    200: Ran 10 train steps in 2.17 secs\n",
      "Step    200: train CrossEntropyLoss |  0.64144927\n",
      "Step    200: eval  CrossEntropyLoss |  0.62195116\n",
      "Step    200: eval          Accuracy |  0.96875000\n",
      "\n",
      "Step    210: Ran 10 train steps in 2.16 secs\n",
      "Step    210: train CrossEntropyLoss |  0.63882172\n",
      "Step    210: eval  CrossEntropyLoss |  0.61118662\n",
      "Step    210: eval          Accuracy |  0.93750000\n",
      "\n",
      "Step    220: Ran 10 train steps in 3.17 secs\n",
      "Step    220: train CrossEntropyLoss |  0.61910957\n",
      "Step    220: eval  CrossEntropyLoss |  0.60297823\n",
      "Step    220: eval          Accuracy |  0.81250000\n",
      "\n",
      "Step    230: Ran 10 train steps in 2.43 secs\n",
      "Step    230: train CrossEntropyLoss |  0.63573730\n",
      "Step    230: eval  CrossEntropyLoss |  0.64078343\n",
      "Step    230: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step    240: Ran 10 train steps in 2.16 secs\n",
      "Step    240: train CrossEntropyLoss |  0.58544791\n",
      "Step    240: eval  CrossEntropyLoss |  0.64271122\n",
      "Step    240: eval          Accuracy |  0.71875000\n",
      "\n",
      "Step    250: Ran 10 train steps in 2.12 secs\n",
      "Step    250: train CrossEntropyLoss |  0.64459014\n",
      "Step    250: eval  CrossEntropyLoss |  0.63676888\n",
      "Step    250: eval          Accuracy |  0.81250000\n",
      "\n",
      "Step    260: Ran 10 train steps in 2.18 secs\n",
      "Step    260: train CrossEntropyLoss |  0.60660487\n",
      "Step    260: eval  CrossEntropyLoss |  0.62601173\n",
      "Step    260: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step    270: Ran 10 train steps in 2.18 secs\n",
      "Step    270: train CrossEntropyLoss |  0.60478038\n",
      "Step    270: eval  CrossEntropyLoss |  0.66854495\n",
      "Step    270: eval          Accuracy |  0.65625000\n",
      "\n",
      "Step    280: Ran 10 train steps in 2.95 secs\n",
      "Step    280: train CrossEntropyLoss |  0.59973735\n",
      "Step    280: eval  CrossEntropyLoss |  0.66064548\n",
      "Step    280: eval          Accuracy |  0.53125000\n",
      "\n",
      "Step    290: Ran 10 train steps in 2.33 secs\n",
      "Step    290: train CrossEntropyLoss |  0.61323333\n",
      "Step    290: eval  CrossEntropyLoss |  0.71065575\n",
      "Step    290: eval          Accuracy |  0.53125000\n",
      "\n",
      "Step    300: Ran 10 train steps in 2.28 secs\n",
      "Step    300: train CrossEntropyLoss |  0.58352327\n",
      "Step    300: eval  CrossEntropyLoss |  0.59479016\n",
      "Step    300: eval          Accuracy |  0.71875000\n",
      "\n",
      "Step    310: Ran 10 train steps in 2.20 secs\n",
      "Step    310: train CrossEntropyLoss |  0.60286438\n",
      "Step    310: eval  CrossEntropyLoss |  0.60467589\n",
      "Step    310: eval          Accuracy |  0.93750000\n",
      "\n",
      "Step    320: Ran 10 train steps in 2.22 secs\n",
      "Step    320: train CrossEntropyLoss |  0.52787006\n",
      "Step    320: eval  CrossEntropyLoss |  0.66134733\n",
      "Step    320: eval          Accuracy |  0.71875000\n",
      "\n",
      "Step    330: Ran 10 train steps in 2.25 secs\n",
      "Step    330: train CrossEntropyLoss |  0.54627639\n",
      "Step    330: eval  CrossEntropyLoss |  0.61681765\n",
      "Step    330: eval          Accuracy |  0.71875000\n",
      "\n",
      "Step    340: Ran 10 train steps in 3.12 secs\n",
      "Step    340: train CrossEntropyLoss |  0.56090069\n",
      "Step    340: eval  CrossEntropyLoss |  0.62102109\n",
      "Step    340: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step    350: Ran 10 train steps in 2.44 secs\n",
      "Step    350: train CrossEntropyLoss |  0.52471942\n",
      "Step    350: eval  CrossEntropyLoss |  0.59795314\n",
      "Step    350: eval          Accuracy |  0.84375000\n",
      "\n",
      "Step    360: Ran 10 train steps in 4.16 secs\n",
      "Step    360: train CrossEntropyLoss |  0.57989848\n",
      "Step    360: eval  CrossEntropyLoss |  0.62106621\n",
      "Step    360: eval          Accuracy |  0.81250000\n",
      "\n",
      "Step    370: Ran 10 train steps in 2.26 secs\n",
      "Step    370: train CrossEntropyLoss |  0.53400934\n",
      "Step    370: eval  CrossEntropyLoss |  0.62151074\n",
      "Step    370: eval          Accuracy |  0.65625000\n",
      "\n",
      "Step    380: Ran 10 train steps in 2.36 secs\n",
      "Step    380: train CrossEntropyLoss |  0.52201068\n",
      "Step    380: eval  CrossEntropyLoss |  0.53734976\n",
      "Step    380: eval          Accuracy |  0.90625000\n",
      "\n",
      "Step    390: Ran 10 train steps in 2.18 secs\n",
      "Step    390: train CrossEntropyLoss |  0.57694548\n",
      "Step    390: eval  CrossEntropyLoss |  0.53554291\n",
      "Step    390: eval          Accuracy |  0.93750000\n",
      "\n",
      "Step    400: Ran 10 train steps in 2.92 secs\n",
      "Step    400: train CrossEntropyLoss |  0.53406054\n",
      "Step    400: eval  CrossEntropyLoss |  0.58113909\n",
      "Step    400: eval          Accuracy |  0.78125000\n",
      "\n",
      "Step    410: Ran 10 train steps in 2.44 secs\n",
      "Step    410: train CrossEntropyLoss |  0.54291844\n",
      "Step    410: eval  CrossEntropyLoss |  0.55519485\n",
      "Step    410: eval          Accuracy |  0.93750000\n",
      "\n",
      "Step    420: Ran 10 train steps in 2.29 secs\n",
      "Step    420: train CrossEntropyLoss |  0.54112345\n",
      "Step    420: eval  CrossEntropyLoss |  0.48940134\n",
      "Step    420: eval          Accuracy |  0.87500000\n",
      "\n",
      "Step    430: Ran 10 train steps in 2.27 secs\n",
      "Step    430: train CrossEntropyLoss |  0.48371330\n",
      "Step    430: eval  CrossEntropyLoss |  0.51431084\n",
      "Step    430: eval          Accuracy |  0.96875000\n",
      "\n",
      "Step    440: Ran 10 train steps in 2.30 secs\n",
      "Step    440: train CrossEntropyLoss |  0.49360719\n",
      "Step    440: eval  CrossEntropyLoss |  0.56157881\n",
      "Step    440: eval          Accuracy |  0.78125000\n",
      "\n",
      "Step    450: Ran 10 train steps in 3.04 secs\n",
      "Step    450: train CrossEntropyLoss |  0.49738544\n",
      "Step    450: eval  CrossEntropyLoss |  0.63042879\n",
      "Step    450: eval          Accuracy |  0.78125000\n",
      "\n",
      "Step    460: Ran 10 train steps in 2.81 secs\n",
      "Step    460: train CrossEntropyLoss |  0.45581484\n",
      "Step    460: eval  CrossEntropyLoss |  0.66379708\n",
      "Step    460: eval          Accuracy |  0.71875000\n",
      "\n",
      "Step    470: Ran 10 train steps in 3.07 secs\n",
      "Step    470: train CrossEntropyLoss |  0.54417866\n",
      "Step    470: eval  CrossEntropyLoss |  0.54153848\n",
      "Step    470: eval          Accuracy |  0.87500000\n",
      "\n",
      "Step    480: Ran 10 train steps in 2.18 secs\n",
      "Step    480: train CrossEntropyLoss |  0.46511215\n",
      "Step    480: eval  CrossEntropyLoss |  0.59169960\n",
      "Step    480: eval          Accuracy |  0.75000000\n",
      "\n",
      "Step    490: Ran 10 train steps in 2.19 secs\n",
      "Step    490: train CrossEntropyLoss |  0.46513349\n",
      "Step    490: eval  CrossEntropyLoss |  0.65129727\n",
      "Step    490: eval          Accuracy |  0.53125000\n",
      "\n",
      "Step    500: Ran 10 train steps in 2.21 secs\n",
      "Step    500: train CrossEntropyLoss |  0.49711770\n",
      "Step    500: eval  CrossEntropyLoss |  0.62975627\n",
      "Step    500: eval          Accuracy |  0.71875000\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(\n",
    "    model=model,\n",
    "    train_task=train_task,\n",
    "    eval_task=[eval_task],\n",
    "    n_steps=500,\n",
    "    output_dir=MODEL_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b9512-af8f-4d81-b222-e9e7d7a33d19",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9457493-9b27-4bf2-8905-68904c4bbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "max_len = all_test_tweets.clean.map(len).max().item()\n",
    "\n",
    "def pad(tweet, max_len):\n",
    "    return tweet + ([0] * (max_len - len(tweet)))\n",
    "\n",
    "all_test_tweets['clean'] = all_test_tweets.clean.apply(pad, args=(max_len,))\n",
    "\n",
    "preds = training_loop.eval_model(np.array(all_test_tweets.clean.to_list()))\n",
    "target = np.array([pred[1] > pred[0] for pred in preds]).astype(np.float32)\n",
    "all_test_tweets['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161d715-d548-4d53-8ea6-744766986621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "all_test_tweets[['id', 'target']].to_csv(f'{OUTPUT_DIR}/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee66474-fe13-4ef7-b798-eb28d320ae69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
